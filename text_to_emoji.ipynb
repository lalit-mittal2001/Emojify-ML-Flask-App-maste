{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6653e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import random\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation,SpatialDropout1D,Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.regularizers import L1L2\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35a363dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text():\n",
    "\tfile = 'raw.pickle'\n",
    "\tresponse = requests.get(\"https://raw.githubusercontent.com/bfelbo/DeepMoji/master/data/PsychExp/raw.pickle\")\n",
    "\topen(file, 'wb').write(response.content)\n",
    "\tdata = pickle.load(open(file,'rb'),encoding='latin1')\n",
    "\tif os.path.exists('data.txt'):\n",
    "\t\tos.remove('data.txt')\n",
    "\ttry:\n",
    "\t\ttexts = [str(x) for x in data['texts']]\n",
    "\t\tlabels = [x['label'] for x in data['info']]\n",
    "\t\twith open(\"data.txt\", 'a') as txtfile: \n",
    "\t\t\tfor i in range(len(texts)):\n",
    "\t\t\t\ttxtfile.write(np.array2string(labels[i]))\n",
    "\t\t\t\ttxtfile.write(str(texts[i])+'\\n')\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\tprint('Oops!! An exception has occurred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e33d9a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_name):\n",
    "\tdata_list  = []\n",
    "\twith open(file_name,'r') as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\tline = line.strip()\n",
    "\t\t\tlabel = ' '.join(line[:line.find(\"]\")].strip().split())\n",
    "\t\t\ttext = line[line.find(\"]\")+1:].strip()\n",
    "\t\t\tdata_list.append([label, text])\n",
    "\n",
    "\treturn data_list\n",
    "\n",
    "def extract_labels(text_list):\n",
    "\tlabel_list = []\n",
    "\ttext_list = [text_list[i][0].replace('[','') for i in range(len(text_list))]\n",
    "\tlabel_list = [list(np.fromstring(text_list[i], dtype=float, sep=' ')) for i in range(len(text_list))]\n",
    "\treturn label_list\n",
    "\n",
    "def extract_text_msgs(text_list):\n",
    "\tmsg_list = []\n",
    "\tmsg_list = [text_list[i][1] for i in range(len(text_list))]\n",
    "\treturn msg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64339dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vector(glove_file):\n",
    "\twith open(glove_file,'r',encoding='UTF-8') as file:\n",
    "\t\twords = set()\n",
    "\t\tword_to_vec = {}\n",
    "\t\tfor line in file:\n",
    "\t\t\tline = line.strip().split()\n",
    "\t\t\tline[0] = re.sub('[^a-zA-Z]', '', line[0])\n",
    "\t\t\tif len(line[0]) > 0:\n",
    "\t\t\t\twords.add(line[0])\n",
    "\t\t\t\tword_to_vec[line[0]] = np.array(line[1:],dtype=np.float64)\n",
    "\n",
    "\t\ti = 1\n",
    "\t\tword_to_index = {}\n",
    "\t\tindex_to_word = {}\n",
    "\t\tfor word in sorted(words):\n",
    "\t\t\tword_to_index[word] = i\n",
    "\t\t\tindex_to_word[i] = word\n",
    "\t\t\ti = i+1\n",
    "\treturn word_to_index,index_to_word,word_to_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1919015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(text_arr,word_to_index,max_len):\n",
    "\tarr_len = text_arr.shape[0]\n",
    "\tarr_indices = np.zeros((arr_len,max_len))\n",
    "\tfor i in range(arr_len):\n",
    "\t\tsentence = text_arr[i].lower().split()\n",
    "\t\tj = 0\n",
    "\t\tfor word in sentence:\n",
    "\t\t\tif word in word_to_index:\n",
    "\t\t\t\tarr_indices[i,j] = word_to_index[word]\n",
    "\t\t\t\tj = j+1\n",
    "\n",
    "\treturn arr_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "214dd48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_layer(word_to_index,word_to_vec):\n",
    "\tcorpus_len = len(word_to_index) + 1\n",
    "\tembed_dim = word_to_vec['word'].shape[0]\n",
    "\n",
    "\tembed_matrix = np.zeros((corpus_len,embed_dim))\n",
    "\n",
    "\tfor word, index in word_to_index.items():\n",
    "\t\tembed_matrix[index,:] = word_to_vec[word]\n",
    "\n",
    "\tembedding_layer = Embedding(corpus_len, embed_dim)\n",
    "\tembedding_layer.build((None,))\n",
    "\tembedding_layer.set_weights([embed_matrix])\n",
    "\n",
    "\treturn embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c102bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(input_shape,embedding_layer):\n",
    "\tsentence_indices = Input(shape=input_shape, dtype=np.int32)\n",
    "\tembedding_layer =  embedding_layer\n",
    "\tembeddings = embedding_layer(sentence_indices)\n",
    "\treg = L1L2(0.01, 0.01)\n",
    "\n",
    "\tX = Bidirectional(LSTM(128, return_sequences=True,bias_regularizer=reg,kernel_initializer='he_uniform'))(embeddings)\n",
    "\tX = BatchNormalization()(X)\n",
    "\tX = Dropout(0.5)(X)\n",
    "\tX = LSTM(64)(X)\n",
    "\tX = Dropout(0.5)(X)\n",
    "\tX = Dense(7, activation='softmax')(X)\n",
    "\tX =  Activation('softmax')(X)\n",
    "\tmodel = Model(sentence_indices, X)\n",
    "\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7be5528c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 50, 50)            17090100  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 50, 256)           183296    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 50, 256)           1024      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                82176     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 7)                 455       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 17,357,051\n",
      "Trainable params: 17,356,539\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "187/187 [==============================] - 61s 305ms/step - loss: 6.3844 - accuracy: 0.1718\n",
      "Epoch 2/30\n",
      "187/187 [==============================] - 63s 336ms/step - loss: 5.1515 - accuracy: 0.2418\n",
      "Epoch 3/30\n",
      "187/187 [==============================] - 58s 308ms/step - loss: 4.1292 - accuracy: 0.3182\n",
      "Epoch 4/30\n",
      "187/187 [==============================] - 58s 310ms/step - loss: 3.2987 - accuracy: 0.3743\n",
      "Epoch 5/30\n",
      "187/187 [==============================] - 62s 331ms/step - loss: 2.6383 - accuracy: 0.4317\n",
      "Epoch 6/30\n",
      "187/187 [==============================] - 61s 326ms/step - loss: 2.1041 - accuracy: 0.5033\n",
      "Epoch 7/30\n",
      "187/187 [==============================] - 58s 310ms/step - loss: 1.7262 - accuracy: 0.5379\n",
      "Epoch 8/30\n",
      "187/187 [==============================] - 57s 307ms/step - loss: 1.5892 - accuracy: 0.5831\n",
      "Epoch 9/30\n",
      "187/187 [==============================] - 66s 352ms/step - loss: 1.5493 - accuracy: 0.6247\n",
      "Epoch 10/30\n",
      "187/187 [==============================] - 71s 382ms/step - loss: 1.5160 - accuracy: 0.6561\n",
      "Epoch 11/30\n",
      "187/187 [==============================] - 63s 336ms/step - loss: 1.4907 - accuracy: 0.6790\n",
      "Epoch 12/30\n",
      "187/187 [==============================] - 63s 337ms/step - loss: 1.4807 - accuracy: 0.6905\n",
      "Epoch 13/30\n",
      "187/187 [==============================] - 62s 333ms/step - loss: 1.4536 - accuracy: 0.7189\n",
      "Epoch 14/30\n",
      "187/187 [==============================] - 55s 295ms/step - loss: 1.4361 - accuracy: 0.7373\n",
      "Epoch 15/30\n",
      "187/187 [==============================] - 55s 292ms/step - loss: 1.4194 - accuracy: 0.7532\n",
      "Epoch 16/30\n",
      "187/187 [==============================] - 58s 310ms/step - loss: 1.4149 - accuracy: 0.7577\n",
      "Epoch 17/30\n",
      "187/187 [==============================] - 68s 360ms/step - loss: 1.3978 - accuracy: 0.7737\n",
      "Epoch 18/30\n",
      "187/187 [==============================] - 61s 325ms/step - loss: 1.3876 - accuracy: 0.7836\n",
      "Epoch 19/30\n",
      "187/187 [==============================] - 56s 300ms/step - loss: 1.3745 - accuracy: 0.7976\n",
      "Epoch 20/30\n",
      "187/187 [==============================] - 59s 314ms/step - loss: 1.3683 - accuracy: 0.8023\n",
      "Epoch 21/30\n",
      "187/187 [==============================] - 60s 320ms/step - loss: 1.3579 - accuracy: 0.8127\n",
      "Epoch 22/30\n",
      "187/187 [==============================] - 63s 338ms/step - loss: 1.3555 - accuracy: 0.8145\n",
      "Epoch 23/30\n",
      "187/187 [==============================] - 59s 313ms/step - loss: 1.3351 - accuracy: 0.8352\n",
      "Epoch 24/30\n",
      "187/187 [==============================] - 84s 449ms/step - loss: 1.3337 - accuracy: 0.8374\n",
      "Epoch 25/30\n",
      "187/187 [==============================] - 76s 405ms/step - loss: 1.3275 - accuracy: 0.8412\n",
      "Epoch 26/30\n",
      "187/187 [==============================] - 77s 409ms/step - loss: 1.3337 - accuracy: 0.8379\n",
      "Epoch 27/30\n",
      "187/187 [==============================] - 78s 418ms/step - loss: 1.3220 - accuracy: 0.8471\n",
      "Epoch 28/30\n",
      "187/187 [==============================] - 56s 300ms/step - loss: 1.3171 - accuracy: 0.8516\n",
      "Epoch 29/30\n",
      "187/187 [==============================] - 64s 345ms/step - loss: 1.3092 - accuracy: 0.8596\n",
      "Epoch 30/30\n",
      "187/187 [==============================] - 65s 349ms/step - loss: 1.3111 - accuracy: 0.8573\n",
      "47/47 [==============================] - 3s 33ms/step - loss: 1.6812 - accuracy: 0.4786\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\textract_text()\n",
    "\ttextlist = read_text_file(\"data.txt\")\n",
    "\tlabel_list = extract_labels(textlist)\n",
    "\tmsg_list = extract_text_msgs(textlist)\n",
    "\tword_to_index,index_to_word,word_to_vec = read_glove_vector('glove.6B.50d.txt')\n",
    "\tx_train, x_test, y_train, y_test = train_test_split(msg_list, label_list,stratify = label_list,\\\n",
    "\t\ttest_size = 0.2, random_state = 123)\n",
    "\ttk = Tokenizer(lower = True, filters='')\n",
    "\ttk.fit_on_texts(msg_list)\n",
    "\ttrain_tokenized = tk.texts_to_sequences(x_train)\n",
    "\ttest_tokenized = tk.texts_to_sequences(x_test)\n",
    "\tmaxlen = 50\n",
    "\tX_train = pad_sequences(train_tokenized, maxlen = maxlen)\n",
    "\tX_test = pad_sequences(test_tokenized, maxlen = maxlen)\n",
    "\tif os.path.exists('tokenizer.pickle'):\n",
    "\t\tos.remove('tokenizer.pickle')\n",
    "\t\twith open('tokenizer.pickle', 'wb') as tokenizer:\n",
    "\t\t\tpickle.dump(tk, tokenizer, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\tembedding_layer = create_embedding_layer(word_to_index,word_to_vec)\n",
    "\tmodel = create_lstm_model((maxlen,),embedding_layer)\n",
    "\tprint(model.summary())\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\tmodel.fit(X_train, np.array(y_train), epochs = 30, batch_size = 32, shuffle=True)\n",
    "\tmodel.save('emoji_model.h5')\n",
    "\tmodel = load_model('emoji_model.h5')\n",
    "\tloss, acc = model.evaluate(X_test, np.array(y_test))\n",
    "\ttest_sent = tk.texts_to_sequences(['Feeling sad that my favourite cricketer has retired'])\n",
    "\ttest_sent = pad_sequences(test_sent, maxlen = maxlen)\n",
    "\tpred = model.predict(test_sent)\n",
    "\tprint(np.argmax(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79d35e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('emoji_model.h5')\n",
    "model = load_model('emoji_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3504b8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
